{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38194408",
   "metadata": {},
   "source": [
    "# 03. 특허 원문 수집 (KIPRIS PDF + Google Patents)\n",
    "\n",
    "이 노트북은 RAG(Retrieval-Augmented Generation) 벡터스토어 구축을 위해 특허의 원문(Full-text) 데이터를 수집하고 전처리합니다.\n",
    "\n",
    "## 주요 기능\n",
    "1. **타겟 특허 (한국) 수집**:\n",
    "   - **KIPRIS Plus API** (`getPubFullTextInfoSearch`)를 사용하여 공개전문 PDF URL을 획득합니다.\n",
    "   - PDF를 다운로드하고 `pypdf` 라이브러리를 사용하여 텍스트를 추출합니다.\n",
    "   - 추출된 텍스트에서 주요 섹션(초록, 명세서, 청구범위)을 파싱합니다.\n",
    "\n",
    "2. **선행기술 (한국 및 해외) 수집**:\n",
    "   - **한국 특허**: KIPRIS PDF 방식을 우선 시도하고, 실패 시 Google Patents 크롤링으로 대체(Fallback)합니다.\n",
    "   - **해외 특허**: Google Patents를 크롤링하여 원문을 확보합니다.\n",
    "   - **다국어 처리**: 한국어, 영어, 중국어, 일본어 등 다국어 텍스트의 인코딩(UTF-8)을 보장합니다.\n",
    "\n",
    "3. **데이터 저장**:\n",
    "   - 수집된 텍스트는 `data/processed/fulltext/` 디렉토리에 구조화된 텍스트 파일로 저장됩니다.\n",
    "   - 메타데이터는 JSON 파일로 저장되어 추후 추적 가능합니다.\n",
    "\n",
    "## 사전 요구사항\n",
    "- `KIPRIS_API_KEY`가 `.env` 파일에 설정되어 있어야 합니다.\n",
    "- 필요한 라이브러리: `requests`, `beautifulsoup4`, `xmltodict`, `pypdf`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75610d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repo Root: /home/arkwith/Dev/paper_data\n",
      "✓ API Key: aggT...\n",
      "✓ Dataset: /home/arkwith/Dev/paper_data/data/processed/kipris_semiconductor_ai_dataset_paper.jsonl\n",
      "✓ Output Dir: /home/arkwith/Dev/paper_data/data/processed/fulltext\n",
      "✓ PDF Cache: /home/arkwith/Dev/paper_data/data/processed/fulltext_pdfs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# 환경 설정\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    cur = (start or Path.cwd()).resolve()\n",
    "    for p in (cur, *cur.parents):\n",
    "        if (p / \"pyproject.toml\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "load_dotenv(REPO_ROOT / \"env\")\n",
    "load_dotenv(REPO_ROOT / \".env\")\n",
    "\n",
    "KIPRIS_API_KEY = os.getenv(\"KIPRIS_API_KEY\", \"\")\n",
    "if not KIPRIS_API_KEY:\n",
    "    raise ValueError(\"KIPRIS_API_KEY가 설정되지 않았습니다. env 파일을 확인하세요.\")\n",
    "\n",
    "# KIPRIS API 설정\n",
    "BASE_URL = \"http://plus.kipris.or.kr/kipo-api/kipi/patUtiModInfoSearchSevice\"\n",
    "PATH_PUB_FULLTEXT = \"getPubFullTextInfoSearch\"  # 공개전문 PDF\n",
    "PATH_ANN_FULLTEXT = \"getAnnFullTextInfoSearch\"  # 공고전문 PDF (공개 실패 시 fallback, applicationNumber 사용)\n",
    "PATH_REG_FULLTEXT = \"getRegFullTextInfoSearch\"  # 등록전문 PDF (추가 fallback; 스펙/파라미터는 상황에 따라 다를 수 있음)\n",
    "\n",
    "# 출력 디렉토리\n",
    "OUT_DIR = REPO_ROOT / \"data\" / \"processed\" / \"fulltext\"\n",
    "PDF_DIR = REPO_ROOT / \"data\" / \"processed\" / \"fulltext_pdfs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 논문 정합성: examiner-only GT(=paper mode)로 구축한 데이터셋을 우선 사용\n",
    "_dataset_candidates = [\n",
    "    REPO_ROOT / \"data\" / \"processed\" / \"kipris_semiconductor_ai_dataset_paper.jsonl\",\n",
    "    REPO_ROOT / \"data\" / \"processed\" / \"kipris_semiconductor_ai_dataset_paper.checkpoint.jsonl\",\n",
    "    REPO_ROOT / \"data\" / \"processed\" / \"kipris_semiconductor_ai_dataset.jsonl\",\n",
    "    REPO_ROOT / \"data\" / \"processed\" / \"kipris_semiconductor_ai_dataset.checkpoint.jsonl\",\n",
    " ]\n",
    "DATASET_PATH = next((p for p in _dataset_candidates if p.exists()), _dataset_candidates[0])\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"데이터셋 파일을 찾지 못했습니다. 먼저 notebooks/02_kipris_dataset_builder.ipynb를 MODE='paper'로 실행해\\n\"\n",
    "        \"data/processed/kipris_semiconductor_ai_dataset_paper.jsonl (또는 checkpoint) 를 생성하세요.\"\n",
    "    )\n",
    "\n",
    "print(f\"✓ Repo Root: {REPO_ROOT}\")\n",
    "print(f\"✓ API Key: {KIPRIS_API_KEY[:4]}...\")\n",
    "print(f\"✓ Dataset: {DATASET_PATH}\")\n",
    "print(f\"✓ Output Dir: {OUT_DIR}\")\n",
    "print(f\"✓ PDF Cache: {PDF_DIR}\")\n",
    "\n",
    "# 수집 설정\n",
    "SAMPLE_SIZE = 100  # 타겟 특허 수집 개수(거절-only 모집단에서 앞에서부터 N개)\n",
    "COHORT = \"reject_only\"  # \"reject_only\" | \"all\"\n",
    "SAMPLING_METHOD = \"head\"  # \"head\"(순차 수집) | \"random\"(실험용)\n",
    "SAMPLING_SEED = 42  # random 샘플 재현성\n",
    "MIN_REQUEST_INTERVAL = 0.5  # KIPRIS 요청 간격(초)\n",
    "GOOGLE_PATENTS_DELAY = 2.0  # Google Patents 요청 간격(초)\n",
    "MAX_WORKERS = 1  # 본 노트북은 기본값으로 '순차 수집'합니다. (병렬은 rate limit 위험)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a9ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ KIPRIS PDF 함수 정의 완료 (PDF 캐시 파일명: <특허번호>.pdf)\n"
     ]
    }
   ],
   "source": [
    "# KIPRIS PDF API 함수 (공개전문→공고전문→(옵션)등록전문 fallback + 실패 사유 수집)\n",
    "\n",
    "_kipris_lock = threading.Lock()\n",
    "_kipris_last_request = 0.0\n",
    "\n",
    "class KiprisServiceKeyError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def _throttle_kipris():\n",
    "    \"\"\"KIPRIS API rate limiting\"\"\"\n",
    "    global _kipris_last_request\n",
    "    with _kipris_lock:\n",
    "        elapsed = time.monotonic() - _kipris_last_request\n",
    "        if elapsed < MIN_REQUEST_INTERVAL:\n",
    "            time.sleep(MIN_REQUEST_INTERVAL - elapsed)\n",
    "        _kipris_last_request = time.monotonic()\n",
    "\n",
    "def kipris_call(path: str, params: Dict[str, Any], max_retries: int = 3) -> tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"KIPRIS API 호출 후 (header, body) 반환.\n",
    "\n",
    "    - 실패하더라도 header/resultCode/resultMsg를 최대한 보존해서 반환\n",
    "    - '서비스키/권한 만료'만 치명적 예외로 처리 (그 외는 실패 사유로 기록하고 계속 진행)\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/{path}\"\n",
    "    params = params.copy()\n",
    "    params[\"ServiceKey\"] = KIPRIS_API_KEY\n",
    "\n",
    "    last_exc: Optional[Exception] = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            _throttle_kipris()\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = xmltodict.parse(resp.text)\n",
    "            response = data.get(\"response\", {}) or {}\n",
    "            header = response.get(\"header\", {}) or {}\n",
    "            body = response.get(\"body\", {}) or {}\n",
    "\n",
    "            if str(header.get(\"successYN\", \"\")).strip().upper() == \"N\":\n",
    "                msg = str(header.get(\"resultMsg\", \"\") or \"\").strip()\n",
    "                # 서비스키/권한 만료만 즉시 중단\n",
    "                if any(x in msg for x in [\"서비스 이용 권한\", \"서비스키\", \"만료\", \"잘못\", \"SERVICEKEY\", \"SERVICE KEY\"]):\n",
    "                    raise KiprisServiceKeyError(msg or \"KIPRIS ServiceKey error\")\n",
    "            return header, body\n",
    "        except KiprisServiceKeyError:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"successYN\": \"N\", \"resultCode\": \"HTTP/CLIENT\", \"resultMsg\": str(e)}, {}\n",
    "            time.sleep(1 * (attempt + 1))\n",
    "\n",
    "    return {\"successYN\": \"N\", \"resultCode\": \"UNKNOWN\", \"resultMsg\": str(last_exc) if last_exc else \"unknown\"}, {}\n",
    "\n",
    "def _extract_pdf_url_from_body(body: Dict[str, Any]) -> Optional[str]:\n",
    "    item = (body.get(\"item\", {}) or {}) if isinstance(body, dict) else {}\n",
    "    if isinstance(item, list):\n",
    "        item = item[0] if item else {}\n",
    "    pdf_url = \"\"\n",
    "    if isinstance(item, dict):\n",
    "        pdf_url = str(item.get(\"path\", \"\") or \"\")\n",
    "    return pdf_url if (pdf_url and pdf_url.startswith(\"http\")) else None\n",
    "\n",
    "def get_kipris_pdf_url_with_reason(*, identifier: str, operation: str, id_param: str) -> tuple[Optional[str], Dict[str, Any]]:\n",
    "    \"\"\"KIPRIS에서 PDF URL을 얻고, 실패 사유(header 등)를 함께 반환.\"\"\"\n",
    "    header, body = kipris_call(operation, {id_param: identifier})\n",
    "    pdf_url = _extract_pdf_url_from_body(body)\n",
    "    info = {\n",
    "        \"operation\": operation,\n",
    "        \"id_param\": id_param,\n",
    "        \"identifier\": identifier,\n",
    "        \"successYN\": str(header.get(\"successYN\", \"\")).strip(),\n",
    "        \"resultCode\": str(header.get(\"resultCode\", \"\")).strip(),\n",
    "        \"resultMsg\": str(header.get(\"resultMsg\", \"\")).strip(),\n",
    "        \"has_body\": bool(body),\n",
    "        \"has_pdf_url\": bool(pdf_url),\n",
    "    }\n",
    "    return pdf_url, info\n",
    "\n",
    "def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> Optional[str]:\n",
    "    try:\n",
    "        reader = PdfReader(BytesIO(pdf_bytes))\n",
    "        text_parts = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "        return full_text if full_text.strip() else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def download_and_extract_pdf(pdf_url: str, cache_path: Optional[Path] = None) -> Optional[str]:\n",
    "    \"\"\"PDF 다운로드 및 텍스트 추출 (cache_path가 있으면 재사용)\"\"\"\n",
    "    try:\n",
    "        if cache_path and cache_path.exists():\n",
    "            return extract_text_from_pdf_bytes(cache_path.read_bytes())\n",
    "\n",
    "        resp = requests.get(pdf_url, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        if cache_path:\n",
    "            cache_path.write_bytes(resp.content)\n",
    "        return extract_text_from_pdf_bytes(resp.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download/extract PDF from {pdf_url[:50]}...: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_patent_sections(text: str, app_num: str) -> Dict[str, str]:\n",
    "    \"\"\"특허 텍스트를 섹션별로 파싱 (간단 휴리스틱)\"\"\"\n",
    "    result = {\n",
    "        \"application_number\": app_num,\n",
    "        \"full_text\": text,\n",
    "        \"abstract\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"claims\": \"\"\n",
    "    }\n",
    "\n",
    "    for keyword in [\"【요약】\", \"【발명의 요약】\", \"[요약]\"]:\n",
    "        if keyword in text:\n",
    "            parts = text.split(keyword)\n",
    "            if len(parts) > 1:\n",
    "                content = parts[1]\n",
    "                if \"【\" in content:\n",
    "                    content = content.split(\"【\")[0]\n",
    "                result[\"abstract\"] = content.strip()[:2000]\n",
    "                break\n",
    "\n",
    "    for keyword in [\"【청구범위】\", \"[청구범위]\", \"【특허청구범위】\"]:\n",
    "        if keyword in text:\n",
    "            parts = text.split(keyword)\n",
    "            if len(parts) > 1:\n",
    "                content = parts[1]\n",
    "                if \"【\" in content:\n",
    "                    content = content.split(\"【\")[0]\n",
    "                result[\"claims\"] = content.strip()\n",
    "                break\n",
    "\n",
    "    for keyword in [\"【발명의 상세한 설명】\", \"[발명의 상세한 설명]\"]:\n",
    "        if keyword in text:\n",
    "            parts = text.split(keyword)\n",
    "            if len(parts) > 1:\n",
    "                content = parts[1]\n",
    "                if \"【\" in content:\n",
    "                    content = content.split(\"【\")[0]\n",
    "                result[\"description\"] = content.strip()\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "def _canonical_pdf_cache_path(cache_stem: str) -> Path:\n",
    "    \"\"\"PDF 캐시 파일명은 '특허번호만.pdf' 형태로 통일.\"\"\"\n",
    "    return PDF_DIR / f\"{cache_stem}.pdf\"\n",
    "\n",
    "def _pick_cache_path(cache_stem: str, operation: str) -> Optional[Path]:\n",
    "    \"\"\"기존 캐시 호환:\n",
    "    1) <stem>.pdf (신규 표준)\n",
    "    2) <stem>.<operation>.pdf (과거 생성 파일)\n",
    "    \"\"\"\n",
    "    p_std = _canonical_pdf_cache_path(cache_stem)\n",
    "    if p_std.exists():\n",
    "        return p_std\n",
    "    p_old = PDF_DIR / f\"{cache_stem}.{operation}.pdf\"\n",
    "    if p_old.exists():\n",
    "        return p_old\n",
    "    return None\n",
    "\n",
    "def _attempt_operation(*, clean_id: str, operation: str, id_param: str, cache_stem: str, application_number: str, tried: list[dict]) -> Optional[Dict[str, str]]:\n",
    "    # 0) 이미 PDF 캐시가 있으면 API 호출 없이 재추출\n",
    "    cache_path = _pick_cache_path(cache_stem, operation)\n",
    "    if cache_path is not None:\n",
    "        text = extract_text_from_pdf_bytes(cache_path.read_bytes())\n",
    "        tried.append({\"operation\": operation, \"resultCode\": \"CACHE\", \"resultMsg\": \"used_cached_pdf\", \"cache_path\": str(cache_path)})\n",
    "        if text:\n",
    "            result = parse_patent_sections(text, application_number)\n",
    "            result[\"source\"] = \"kipris_pdf\"\n",
    "            result[\"kipris_operation\"] = operation\n",
    "            result[\"pdf_url\"] = str(cache_path)\n",
    "            return result\n",
    "        tried.append({\"operation\": operation, \"resultCode\": \"PDF\", \"resultMsg\": \"cached_pdf_extract_failed\", \"cache_path\": str(cache_path)})\n",
    "        # 캐시가 깨졌을 수도 있으니 계속해서 API 시도\n",
    "    else:\n",
    "        tried.append({\"operation\": operation, \"resultCode\": \"CACHE\", \"resultMsg\": \"no_cached_pdf\"})\n",
    "\n",
    "    # 1) API로 PDF URL 획득 → 다운로드/캐시/추출\n",
    "    pdf_url, info = get_kipris_pdf_url_with_reason(identifier=clean_id, operation=operation, id_param=id_param)\n",
    "    tried.append(info)\n",
    "    if not pdf_url:\n",
    "        return None\n",
    "\n",
    "    # 신규 표준 파일명: <특허번호>.pdf\n",
    "    cache_path = _canonical_pdf_cache_path(cache_stem)\n",
    "    text = download_and_extract_pdf(pdf_url, cache_path)\n",
    "    if not text:\n",
    "        tried.append({\"operation\": operation, \"resultCode\": \"PDF\", \"resultMsg\": \"download_or_extract_failed\"})\n",
    "        return None\n",
    "\n",
    "    result = parse_patent_sections(text, application_number)\n",
    "    result[\"source\"] = \"kipris_pdf\"\n",
    "    result[\"kipris_operation\"] = operation\n",
    "    result[\"pdf_url\"] = pdf_url\n",
    "    return result\n",
    "\n",
    "def get_kipris_fulltext_with_reason(application_number: str, *, registration_number: Optional[str] = None) -> tuple[Optional[Dict[str, str]], Dict[str, Any]]:\n",
    "    \"\"\"KIPRIS PDF 파이프라인:\n",
    "    공개전문(applicationNumber) → 공고전문(applicationNumber) → (옵션)등록전문(registrationNumber) fallback + 실패 사유 반환.\n",
    "\n",
    "    최적화: PDF 캐시가 있으면 API 호출 없이 재추출합니다.\n",
    "    \"\"\"\n",
    "    clean_app_num = re.sub(r\"[^0-9]\", \"\", str(application_number))\n",
    "    clean_reg_num = re.sub(r\"[^0-9]\", \"\", str(registration_number)) if registration_number else \"\"\n",
    "    cache_stem = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", clean_app_num)\n",
    "    tried: list[dict] = []\n",
    "    used_operation: Optional[str] = None\n",
    "\n",
    "    # 1) 공개전문: applicationNumber\n",
    "    result = _attempt_operation(\n",
    "        clean_id=clean_app_num,\n",
    "        operation=PATH_PUB_FULLTEXT,\n",
    "        id_param=\"applicationNumber\",\n",
    "        cache_stem=cache_stem,\n",
    "        application_number=application_number,\n",
    "        tried=tried,\n",
    "    )\n",
    "    if result:\n",
    "        used_operation = PATH_PUB_FULLTEXT\n",
    "        return result, {\"ok\": True, \"tried\": tried, \"used_operation\": used_operation}\n",
    "\n",
    "    # 2) 공고전문: applicationNumber\n",
    "    result = _attempt_operation(\n",
    "        clean_id=clean_app_num,\n",
    "        operation=PATH_ANN_FULLTEXT,\n",
    "        id_param=\"applicationNumber\",\n",
    "        cache_stem=cache_stem,\n",
    "        application_number=application_number,\n",
    "        tried=tried,\n",
    "    )\n",
    "    if result:\n",
    "        used_operation = PATH_ANN_FULLTEXT\n",
    "        return result, {\"ok\": True, \"tried\": tried, \"used_operation\": used_operation}\n",
    "\n",
    "    # 3) (옵션) 등록전문: registrationNumber (없으면 스킵)\n",
    "    if clean_reg_num:\n",
    "        result = _attempt_operation(\n",
    "            clean_id=clean_reg_num,\n",
    "            operation=PATH_REG_FULLTEXT,\n",
    "            id_param=\"registrationNumber\",\n",
    "            cache_stem=cache_stem,\n",
    "            application_number=application_number,\n",
    "            tried=tried,\n",
    "        )\n",
    "        if result:\n",
    "            used_operation = PATH_REG_FULLTEXT\n",
    "            return result, {\"ok\": True, \"tried\": tried, \"used_operation\": used_operation}\n",
    "    else:\n",
    "        tried.append({\"operation\": PATH_REG_FULLTEXT, \"resultCode\": \"SKIP\", \"resultMsg\": \"no_registration_number\"})\n",
    "\n",
    "    return None, {\"ok\": False, \"tried\": tried, \"used_operation\": used_operation}\n",
    "\n",
    "# 기존 코드 호환용 (기존 호출부가 있으면 동작)\n",
    "def get_kipris_fulltext(application_number: str) -> Optional[Dict[str, str]]:\n",
    "    result, _ = get_kipris_fulltext_with_reason(application_number)\n",
    "    return result\n",
    "\n",
    "print(\"✓ KIPRIS PDF 함수 정의 완료 (PDF 캐시 파일명: <특허번호>.pdf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b977ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Google Patents 크롤링 함수 정의 완료 (문단/인코딩 보존 강화)\n"
     ]
    }
   ],
   "source": [
    "# Google Patents 크롤링 함수 (인코딩/정규화/비특허(NPL) 필터 보강 + 섹션/문단 보존 강화)\n",
    "\n",
    "_google_last_request = 0.0\n",
    "_google_lock = threading.Lock()\n",
    "\n",
    "# 특허 문서번호처럼 보이는지(간단 휴리스틱)\n",
    "_PATENT_PREFIXES = (\n",
    "    \"US\", \"KR\", \"JP\", \"CN\", \"EP\", \"WO\", \"DE\", \"FR\", \"GB\", \"TW\",\n",
    "    \"CA\", \"AU\", \"RU\", \"IN\", \"BR\", \"IT\", \"ES\"\n",
    ")\n",
    "\n",
    "\n",
    "def _throttle_google():\n",
    "    \"\"\"Google Patents rate limiting\"\"\"\n",
    "    global _google_last_request\n",
    "    with _google_lock:\n",
    "        elapsed = time.monotonic() - _google_last_request\n",
    "        if elapsed < GOOGLE_PATENTS_DELAY:\n",
    "            time.sleep(GOOGLE_PATENTS_DELAY - elapsed)\n",
    "        _google_last_request = time.monotonic()\n",
    "\n",
    "\n",
    "def looks_like_patent_doc_number(value: str) -> bool:\n",
    "    \"\"\"선행기술 문자열이 특허 문서번호 형태인지 판별.\n",
    "\n",
    "    - 논문/문헌(NPL)은 Google Patents로 가져올 수 없어 404가 다량 발생하므로 미리 제외\n",
    "    - 완벽한 규칙이 아니라 '오류 감소' 목적의 휴리스틱\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return False\n",
    "\n",
    "    s = str(value).strip()\n",
    "\n",
    "    # 아주 흔한 NPL 패턴\n",
    "    if any(x in s for x in [\"arXiv\", \"Vol.\", \"pp.\", \"doi\", \"et al\", \"IEEE\", \"ACM\", \"Journal\", \"Proceedings\"]):\n",
    "        return False\n",
    "\n",
    "    # JP평성... 같은 비정형 라벨 제외\n",
    "    if re.search(r\"[가-힣]\", s) and not s.upper().startswith(\"KR\"):\n",
    "        return False\n",
    "\n",
    "    up = s.upper().replace(\" \", \"\")\n",
    "    if not up.startswith(_PATENT_PREFIXES):\n",
    "        return False\n",
    "\n",
    "    # 숫자가 충분히 있어야 함\n",
    "    digits = re.findall(r\"\\d+\", up)\n",
    "    if not digits:\n",
    "        return False\n",
    "    if len(\"\".join(digits)) < 6:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def normalize_patent_number(doc_num: str) -> str:\n",
    "    \"\"\"문서번호를 Google Patents URL 형식으로 정규화\n",
    "\n",
    "    목표:\n",
    "    - 공백/슬래시/하이픈 제거\n",
    "    - 'KR10...' → 'KR...' 변환\n",
    "    - 'KR102430483 B1' 같이 중간 공백이 있는 kind code도 결합\n",
    "    \"\"\"\n",
    "    s = str(doc_num).strip()\n",
    "\n",
    "    # 슬래시/공백/하이픈 제거\n",
    "    normalized = re.sub(r\"[\\s/\\-]\", \"\", s)\n",
    "\n",
    "    # 괄호/따옴표 등 URL에 불필요한 문자 제거 (영숫자만 남김)\n",
    "    normalized = re.sub(r\"[^0-9A-Za-z]\", \"\", normalized)\n",
    "\n",
    "    # KR10 → KR (한국 특허 정규화)\n",
    "    normalized = re.sub(r\"^KR10\", \"KR\", normalized, flags=re.IGNORECASE)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def is_korean_patent(doc_num: str) -> bool:\n",
    "    \"\"\"한국 특허인지 판별\"\"\"\n",
    "    return str(doc_num).strip().upper().startswith(\"KR\")\n",
    "\n",
    "\n",
    "def guess_google_patents_lang(doc_num: str) -> str:\n",
    "    \"\"\"문서번호 접두어로 Google Patents 언어 경로를 추정.\n",
    "\n",
    "    - JP/CN/KR은 해당 언어로 먼저 시도하면 CJK 텍스트 보존/표시가 더 안정적인 경우가 있음\n",
    "    \"\"\"\n",
    "    up = str(doc_num).strip().upper()\n",
    "    if up.startswith(\"JP\"):\n",
    "        return \"ja\"\n",
    "    if up.startswith(\"CN\"):\n",
    "        return \"zh-CN\"\n",
    "    if up.startswith(\"KR\"):\n",
    "        return \"ko\"\n",
    "    return \"en\"\n",
    "\n",
    "\n",
    "def _repair_common_mojibake(text: str) -> str:\n",
    "    \"\"\"자주 관측되는 'UTF-8을 latin-1로 잘못 디코딩' 패턴 복원.\n",
    "\n",
    "    예: '；' → bytes(ef bc 9b) → latin-1 디코딩 시 'ï¼'\n",
    "\n",
    "    주의: 항상 안전한 변환은 아니므로, 복원이 실패하면 원문 유지.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    if any(ch in text for ch in [\"ï\", \"ã\", \"â\"]):\n",
    "        try:\n",
    "            fixed = text.encode(\"latin1\").decode(\"utf-8\")\n",
    "            # 복원 결과가 더 길거나 CJK 비율이 증가하는 경우만 채택\n",
    "            mojibake_markers = sum(text.count(x) for x in [\"ï\", \"ã\", \"â\"]) \n",
    "            fixed_markers = sum(fixed.count(x) for x in [\"ï\", \"ã\", \"â\"]) \n",
    "            if fixed_markers < mojibake_markers:\n",
    "                return fixed\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def _decode_html_bytes(raw: bytes) -> str:\n",
    "    # 1) 먼저 UTF-8로 시도 (대부분 Google Patents는 UTF-8)\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\"]:\n",
    "        try:\n",
    "            s = raw.decode(enc)\n",
    "            if \"<html\" in s.lower() or \"<!doctype\" in s.lower():\n",
    "                return s\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) UnicodeDammit에 CJK 후보 인코딩을 알려주고 디코딩\n",
    "    dammit = UnicodeDammit(\n",
    "        raw,\n",
    "        is_html=True,\n",
    "        override_encodings=[\n",
    "            \"utf-8\",\n",
    "            \"gb18030\", \"gbk\",\n",
    "            \"big5\",\n",
    "            \"shift_jis\", \"euc-jp\",\n",
    "            \"cp949\",\n",
    "        ],\n",
    "    )\n",
    "    if dammit.unicode_markup:\n",
    "        return dammit.unicode_markup\n",
    "\n",
    "    # 3) 최후 fallback\n",
    "    return raw.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "def _fetch_google_patents_html(url: str) -> Optional[str]:\n",
    "    \"\"\"URL을 가져와 HTML 문자열로 반환.\n",
    "\n",
    "    중요:\n",
    "    - CJK(한/중/일) 텍스트가 깨지는 경우가 많아, `resp.apparent_encoding`을 강제로 쓰면\n",
    "      오히려 모지바케(예: ë°...)가 발생할 수 있음.\n",
    "    - bytes(resp.content) 기준으로 디코딩 후, 섹션 텍스트를 후처리에서 추가 복원한다.\n",
    "    \"\"\"\n",
    "    _throttle_google()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=30)\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    return _decode_html_bytes(resp.content)\n",
    "\n",
    "\n",
    "_BLOCK_TAGS = (\"p\", \"div\", \"li\", \"tr\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\")\n",
    "\n",
    "\n",
    "def _postprocess_google_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = _repair_common_mojibake(text)\n",
    "\n",
    "    # 'AbstractA ...', 'DescriptionMultitask ...' 형태로 붙어버리는 케이스 보정\n",
    "    text = re.sub(r\"^(Abstract)(?=\\S)\", r\"Abstract\\n\", text)\n",
    "    text = re.sub(r\"^(Description)(?=\\S)\", r\"Description\\n\", text)\n",
    "\n",
    "    # 번역/기계추출 텍스트에서 자주 등장하는 헤딩을 줄바꿈으로 분리\n",
    "    heading_words = [\n",
    "        \"Technical field\",\n",
    "        \"FIELD OF THE INVENTION\",\n",
    "        \"Background technology\",\n",
    "        \"BACKGROUND\",\n",
    "        \"SUMMARY\",\n",
    "        \"The content of the invention\",\n",
    "        \"DETAILED DESCRIPTION\",\n",
    "        \"BRIEF DESCRIPTION OF THE DRAWINGS\",\n",
    "        \"DESCRIPTION OF THE DRAWINGS\",\n",
    "    ]\n",
    "    for w in heading_words:\n",
    "        # 이미 줄 시작이면 유지, 단어 앞에 글자가 붙어있으면 줄바꿈 삽입\n",
    "        text = re.sub(rf\"(?i)(?<!\\n)({re.escape(w)})\\b\", r\"\\n\\1\", text)\n",
    "\n",
    "    # 과도한 공백/빈 줄 정리\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _extract_google_section_text(section_tag) -> str:\n",
    "    if not section_tag:\n",
    "        return \"\"\n",
    "\n",
    "    # 태그를 문자열로 새로 파싱해 조작(원 soup 변경 방지)\n",
    "    frag = BeautifulSoup(str(section_tag), \"html.parser\")\n",
    "\n",
    "    for t in frag([\"script\", \"style\", \"noscript\"]):\n",
    "        t.decompose()\n",
    "\n",
    "    for br in frag.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    for tag in frag.find_all(_BLOCK_TAGS):\n",
    "        tag.insert_before(\"\\n\")\n",
    "        tag.insert_after(\"\\n\")\n",
    "\n",
    "    text = frag.get_text()\n",
    "    return _postprocess_google_text(text)\n",
    "\n",
    "\n",
    "def get_google_patents_fulltext(doc_num: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"Google Patents에서 특허 원문 크롤링\n",
    "\n",
    "    강화 포인트:\n",
    "    - CJK 문서에서 인코딩 모지바케 복원 시도\n",
    "    - 섹션 내부 문단/블록 경계를 최대한 보존 (chunking/RAG 성능에 중요)\n",
    "    \"\"\"\n",
    "    if not looks_like_patent_doc_number(doc_num):\n",
    "        return None\n",
    "\n",
    "    normalized = normalize_patent_number(doc_num)\n",
    "    lang = guess_google_patents_lang(doc_num)\n",
    "\n",
    "    # URL 후보: (1) 언어 경로, (2) /en, (3) 기본\n",
    "    url_candidates = [\n",
    "        f\"https://patents.google.com/patent/{normalized}/{lang}\",\n",
    "        f\"https://patents.google.com/patent/{normalized}/en\",\n",
    "        f\"https://patents.google.com/patent/{normalized}\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        html = None\n",
    "        used_url = None\n",
    "        for u in url_candidates:\n",
    "            html = _fetch_google_patents_html(u)\n",
    "            if html:\n",
    "                used_url = u\n",
    "                break\n",
    "\n",
    "        if not html:\n",
    "            # 특허로 보이는 값만 제한적으로 로그\n",
    "            print(f\"Failed to fetch {doc_num}: HTTP error / not found\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 제목\n",
    "        title_tag = soup.find(\"meta\", {\"name\": \"DC.title\"})\n",
    "        title = title_tag[\"content\"] if title_tag and title_tag.get(\"content\") else \"\"\n",
    "        title = _postprocess_google_text(title)\n",
    "\n",
    "        # Abstract / Description / Claims\n",
    "        abstract_section = soup.find(\"section\", {\"itemprop\": \"abstract\"})\n",
    "        abstract = _extract_google_section_text(abstract_section)\n",
    "\n",
    "        description_section = soup.find(\"section\", {\"itemprop\": \"description\"})\n",
    "        description = _extract_google_section_text(description_section)\n",
    "\n",
    "        claims_section = soup.find(\"section\", {\"itemprop\": \"claims\"})\n",
    "        claims = _extract_google_section_text(claims_section)\n",
    "\n",
    "        if not any([abstract, description, claims]):\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"doc_number\": str(doc_num),\n",
    "            \"normalized\": normalized,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"description\": description,\n",
    "            \"claims\": claims,\n",
    "            \"source\": \"google_patents\",\n",
    "            \"google_url\": used_url or url_candidates[0],\n",
    "            \"google_lang\": lang,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {doc_num}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Google Patents 크롤링 함수 정의 완료 (문단/인코딩 보존 강화)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf55a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로드 중: /home/arkwith/Dev/paper_data/data/processed/kipris_semiconductor_ai_dataset_paper.jsonl\n",
      "✓ 전체 데이터셋: 1500건\n",
      "✓ 거절-only cohort: 194건\n",
      "✓ 샘플링 완료(head): 100건\n",
      "✓ 타겟 특허: 100건\n",
      "  - 등록번호 보유: 0건 (등록전문 fallback 가능)\n",
      "✓ 선행기술(원본, 중복 제거): 273건\n",
      "  - 특허 문서번호로 판별됨: 254건\n",
      "  - 비특허 문헌(NPL)로 판별됨: 19건\n",
      "  - 한국 특허(특허만): 177건\n",
      "  - 해외 특허(특허만): 77건\n",
      "\n",
      "예상 API 호출(집계 기준; 실제는 캐시/중복/순차 처리로 감소할 수 있음):\n",
      "  - KIPRIS: 277건 (약 2.3분)\n",
      "  - Google Patents: 77건 (약 2.6분)\n",
      "\n",
      "(참고) NPL 예시 3개: ['A. Sinha et al., Deep Learning 3D Shape Surfaces Using Geometry Images, ECCV 2016, Lecture Notes in Computer Science, Vol.9910(2016)', 'Aurelijus Vaitkevicius 외, Recognition of American Sign Language Gestures in a Virtual Reality Using Leap Motion 외, Applied Sciences, 2019.01.28., Vol.9, No.445, pp.1-16.', 'Comparison of glaucoma-diagnostic ability between wide-field swept-source OCT retinal nerve fiber layer maps and spectraldomain OCT, Eye (2018) 32:1483-1492']\n",
      "\n",
      "[캐시 상태]\n",
      "- 타겟 캐시: 0/100\n",
      "- 선행기술 캐시(특허만): 0/254\n",
      "- NPL 후보: 19 (수집 단계에서 citation/web로 저장)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 및 샘플링 (거절-only 기본값 + 순차 100건)\n",
    "\n",
    "print(f\"데이터셋 로드 중: {DATASET_PATH}\")\n",
    "\n",
    "dataset: List[Dict[str, Any]] = []\n",
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            dataset.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "print(f\"✓ 전체 데이터셋: {len(dataset)}건\")\n",
    "\n",
    "def _get_reg_fields(entry: Dict[str, Any]) -> tuple[str, str, Optional[bool]]:\n",
    "    tp = entry.get(\"target_patent\", {}) or {}\n",
    "    reg = (((tp.get(\"biblio\") or {}).get(\"registration\") or {}) if isinstance(tp, dict) else {})\n",
    "    status = str(reg.get(\"register_status\") or \"\").strip()\n",
    "    final_disposal = str(reg.get(\"final_disposal\") or \"\").strip()\n",
    "    is_registered = reg.get(\"is_registered\")\n",
    "    # is_registered는 bool 또는 None\n",
    "    return status, final_disposal, is_registered if isinstance(is_registered, bool) else None\n",
    "\n",
    "def _is_reject_only(entry: Dict[str, Any]) -> bool:\n",
    "    # '거절-only'의 최소 정의: register_status 또는 final_disposal에 '거절'이 포함되는 경우\n",
    "    status, final_disposal, _ = _get_reg_fields(entry)\n",
    "    return (\"거절\" in status) or (\"거절\" in final_disposal)\n",
    "\n",
    "# 모집단(cohort) 필터\n",
    "if COHORT == \"reject_only\":\n",
    "    cohort = [e for e in dataset if _is_reject_only(e)]\n",
    "    print(f\"✓ 거절-only cohort: {len(cohort)}건\")\n",
    "else:\n",
    "    cohort = list(dataset)\n",
    "    print(f\"✓ cohort(all): {len(cohort)}건\")\n",
    "\n",
    "# 샘플링: 기본은 순차(head)로 100건 (재현성: app_num 정렬)\n",
    "def _app_num_key(e: Dict[str, Any]) -> str:\n",
    "    tp = e.get(\"target_patent\", {}) or {}\n",
    "    return str(tp.get(\"application_number\") or \"\").strip()\n",
    "\n",
    "cohort_sorted = sorted(cohort, key=_app_num_key)\n",
    "\n",
    "if SAMPLING_METHOD == \"random\":\n",
    "    random.seed(SAMPLING_SEED)\n",
    "    sample = random.sample(cohort_sorted, min(SAMPLE_SIZE, len(cohort_sorted)))\n",
    "    print(f\"✓ 샘플링 완료(random, seed={SAMPLING_SEED}): {len(sample)}건\")\n",
    "else:\n",
    "    sample = cohort_sorted[: min(SAMPLE_SIZE, len(cohort_sorted))]\n",
    "    print(f\"✓ 샘플링 완료(head): {len(sample)}건\")\n",
    "\n",
    "# 수집 대상 특허 번호/등록번호 추출 + 선행기술(특허/비특허) 원본 목록 확보\n",
    "target_app_nums: List[str] = []\n",
    "target_reg_num_by_app: Dict[str, str] = {}\n",
    "prior_art_values: List[str] = []\n",
    "\n",
    "for entry in sample:\n",
    "    tp = entry.get(\"target_patent\", {}) or {}\n",
    "    app_num = str(tp.get(\"application_number\", \"\")).strip()\n",
    "    if not app_num:\n",
    "        continue\n",
    "    target_app_nums.append(app_num)\n",
    "\n",
    "    reg_num = (((tp.get(\"biblio\") or {}).get(\"registration\") or {}).get(\"register_number\"))\n",
    "    if reg_num:\n",
    "        target_reg_num_by_app[app_num] = re.sub(r\"[^0-9]\", \"\", str(reg_num))\n",
    "\n",
    "    gt = entry.get(\"ground_truth_prior_arts\") or []\n",
    "    for pa in gt:\n",
    "        prior_art_values.append(str(pa))\n",
    "\n",
    "# 선행기술 중 특허 문서번호만 추출 (NPL은 별도 트랙으로 '수집(저장)' 처리)\n",
    "prior_art_patent_doc_nums = sorted({v for v in prior_art_values if looks_like_patent_doc_number(v)})\n",
    "prior_art_npl_values = sorted({v for v in prior_art_values if not looks_like_patent_doc_number(v)})\n",
    "\n",
    "print(f\"✓ 타겟 특허: {len(target_app_nums)}건\")\n",
    "print(f\"  - 등록번호 보유: {len(target_reg_num_by_app)}건 (등록전문 fallback 가능)\")\n",
    "print(f\"✓ 선행기술(원본, 중복 제거): {len(set(prior_art_values))}건\")\n",
    "print(f\"  - 특허 문서번호로 판별됨: {len(prior_art_patent_doc_nums)}건\")\n",
    "print(f\"  - 비특허 문헌(NPL)로 판별됨: {len(prior_art_npl_values)}건\")\n",
    "\n",
    "# 한국/해외 특허 분류(집계용)\n",
    "korean_prior_arts = [doc for doc in prior_art_patent_doc_nums if is_korean_patent(doc)]\n",
    "foreign_prior_arts = [doc for doc in prior_art_patent_doc_nums if not is_korean_patent(doc)]\n",
    "\n",
    "print(f\"  - 한국 특허(특허만): {len(korean_prior_arts)}건\")\n",
    "print(f\"  - 해외 특허(특허만): {len(foreign_prior_arts)}건\")\n",
    "\n",
    "total_kipris_calls = len(target_app_nums) + len(korean_prior_arts)\n",
    "print(f\"\\n예상 API 호출(집계 기준; 실제는 캐시/중복/순차 처리로 감소할 수 있음):\")\n",
    "print(f\"  - KIPRIS: {total_kipris_calls}건 (약 {total_kipris_calls * MIN_REQUEST_INTERVAL / 60:.1f}분)\")\n",
    "print(f\"  - Google Patents: {len(foreign_prior_arts)}건 (약 {len(foreign_prior_arts) * GOOGLE_PATENTS_DELAY / 60:.1f}분)\")\n",
    "\n",
    "if prior_art_npl_values:\n",
    "    print(f\"\\n(참고) NPL 예시 3개: {prior_art_npl_values[:3]}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (최적화) 이미 저장된 원문(txt)이 있으면 로드해서 API 호출을 줄임\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "target_dir = OUT_DIR / \"targets\"\n",
    "prior_art_dir = OUT_DIR / \"prior_arts\"\n",
    "npl_dir = OUT_DIR / \"npl\"\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "prior_art_dir.mkdir(parents=True, exist_ok=True)\n",
    "npl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_stem(value: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", str(value))\n",
    "\n",
    "def load_saved_patent_text_txt(path: Path) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"save_patent_text()로 저장한 txt를 다시 dict로 복원.\n",
    "\n",
    "    섹션 마커:\n",
    "    - ## ABSTRACT / ## DESCRIPTION / ## CLAIMS / ## FULL TEXT (FROM PDF)\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        raw = path.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # 기본 메타 헤더\n",
    "    doc_id = \"\"\n",
    "    source = \"\"\n",
    "    kipris_operation = \"\"\n",
    "    title = \"\"\n",
    "\n",
    "    lines = raw.splitlines()\n",
    "    for line in lines[:30]:\n",
    "        if line.startswith(\"Document Number:\"):\n",
    "            doc_id = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"Source:\"):\n",
    "            source = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"KIPRIS Operation:\"):\n",
    "            kipris_operation = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"Title:\"):\n",
    "            title = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    def _extract_section(marker: str) -> str:\n",
    "        m = re.search(rf\"^## {re.escape(marker)}\\s*$\", raw, flags=re.MULTILINE)\n",
    "        if not m:\n",
    "            return \"\"\n",
    "        start = m.end()\n",
    "        # 다음 섹션 헤더 또는 EOF\n",
    "        n = re.search(r\"^## \", raw[start:], flags=re.MULTILINE)\n",
    "        end = start + (n.start() if n else len(raw[start:]))\n",
    "        return raw[start:end].strip()\n",
    "\n",
    "    abstract = _extract_section(\"ABSTRACT\")\n",
    "    description = _extract_section(\"DESCRIPTION\")\n",
    "    claims = _extract_section(\"CLAIMS\")\n",
    "    full_text = _extract_section(\"FULL TEXT (FROM PDF)\")\n",
    "\n",
    "    # 최소한 하나는 있어야 유효\n",
    "    if not any([abstract, description, claims, full_text]):\n",
    "        return None\n",
    "\n",
    "    data: Dict[str, str] = {\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"description\": description,\n",
    "        \"claims\": claims,\n",
    "        \"full_text\": full_text,\n",
    "        \"source\": source or \"cached_txt\",\n",
    "    }\n",
    "    if kipris_operation:\n",
    "        data[\"kipris_operation\"] = kipris_operation\n",
    "    # 타겟은 application_number, 선행기술은 doc_number로도 쓸 수 있음\n",
    "    if doc_id:\n",
    "        if doc_id.isdigit():\n",
    "            data[\"application_number\"] = doc_id\n",
    "        else:\n",
    "            data[\"doc_number\"] = doc_id\n",
    "    return data\n",
    "\n",
    "def load_saved_npl_txt(path: Path) -> Optional[Dict[str, str]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        raw = path.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    if not raw.strip():\n",
    "        return None\n",
    "    return {\"raw\": raw}\n",
    "\n",
    "# 캐시 인덱스\n",
    "cached_target_text_by_app: Dict[str, Dict[str, str]] = {}\n",
    "cached_prior_art_text_by_doc: Dict[str, Dict[str, str]] = {}\n",
    "cached_npl_by_key: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "for app in target_app_nums:\n",
    "    p = target_dir / f\"{_safe_stem(app)}.txt\"\n",
    "    d = load_saved_patent_text_txt(p)\n",
    "    if d:\n",
    "        cached_target_text_by_app[str(app)] = d\n",
    "\n",
    "for doc in prior_art_patent_doc_nums:\n",
    "    p = prior_art_dir / f\"{_safe_stem(doc)}.txt\"\n",
    "    d = load_saved_patent_text_txt(p)\n",
    "    if d:\n",
    "        cached_prior_art_text_by_doc[str(doc)] = d\n",
    "\n",
    "# NPL 캐시는 파일명 규칙이 해시 기반이어서, 여기서는 로드를 강제하지 않고 수집 단계에서 on-demand로 처리\n",
    "print(\"\\n[캐시 상태]\")\n",
    "print(f\"- 타겟 캐시: {len(cached_target_text_by_app)}/{len(target_app_nums)}\")\n",
    "print(f\"- 선행기술 캐시(특허만): {len(cached_prior_art_text_by_doc)}/{len(prior_art_patent_doc_nums)}\")\n",
    "print(f\"- NPL 후보: {len(prior_art_npl_values)} (수집 단계에서 citation/web로 저장)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac55d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KIPRIS 진단: PDF URL/텍스트 추출 확인 ===\n",
      "\n",
      "---\n",
      "test_app 1020210057220\n",
      "ok True\n",
      "tried [{'operation': 'getPubFullTextInfoSearch', 'resultCode': 'CACHE', 'resultMsg': 'no_cached_pdf'}, {'operation': 'getPubFullTextInfoSearch', 'id_param': 'applicationNumber', 'identifier': '1020210057220', 'successYN': 'Y', 'resultCode': '00', 'resultMsg': 'NORMAL SERVICE.', 'has_body': True, 'has_pdf_url': True}]\n",
      "used_operation getPubFullTextInfoSearch\n",
      "extracted_chars 20225\n",
      "text_head (19) 대한민국특허청(KR) (12) 공개특허공보(A) (11) 공개번호   10-2022-0150031 (43) 공개일자   2022년11월10일 (51) 국제특허분류(Int. Cl.)      G01N 29/44 (2006.01)  G01N 29/06 (2006.01)      G01N 29/36 (2006.01)  G06N 3/08 (2006.01)\n",
      "\n",
      "---\n",
      "test_app 1020050050026\n",
      "ok True\n",
      "tried [{'operation': 'getPubFullTextInfoSearch', 'resultCode': 'CACHE', 'resultMsg': 'no_cached_pdf'}, {'operation': 'getPubFullTextInfoSearch', 'id_param': 'applicationNumber', 'identifier': '1020050050026', 'successYN': 'Y', 'resultCode': '00', 'resultMsg': 'NORMAL SERVICE.', 'has_body': True, 'has_pdf_url': True}]\n",
      "used_operation getPubFullTextInfoSearch\n",
      "extracted_chars 6084\n",
      "text_head (19)대한민국특허청(KR) (12) 공개특허공보(A)   (51) 。Int. Cl.      G10L 11/00 (2006.01) (11) 공개번호 (43) 공개일자 10-2006-0128519 2006년12월14일 (21) 출원번호 10-2005-0050026 (22) 출원일자 2005년06월10일 심사청구일자 없음 (71) 출원인 엘지전자 주식회사 서\n"
     ]
    }
   ],
   "source": [
    "# (진단) KIPRIS PDF URL/텍스트 추출 확인 (공개→공고→(옵션)등록 fallback + 사유 출력)\n",
    "print(\"\\n=== KIPRIS 진단: PDF URL/텍스트 추출 확인 ===\")\n",
    "\n",
    "test_apps = [\n",
    "    \"1020210057220\",  # 이전 로그 샘플\n",
    "    \"1020050050026\",  # 공고전문PDF(getAnnFullTextInfoSearch) 문서 스펙 샘플\n",
    " ]\n",
    "\n",
    "for test_app in test_apps:\n",
    "    print(\"\\n---\")\n",
    "    print(\"test_app\", test_app)\n",
    "    try:\n",
    "        result, info = get_kipris_fulltext_with_reason(test_app)\n",
    "        print(\"ok\", info.get(\"ok\"))\n",
    "        print(\"tried\", info.get(\"tried\"))\n",
    "        if result:\n",
    "            print(\"used_operation\", result.get(\"kipris_operation\"))\n",
    "            print(\"extracted_chars\", len(result.get(\"full_text\", \"\")))\n",
    "            print(\"text_head\", (result.get(\"full_text\", \"\")[:200]).replace(\"\\n\", \" \"))\n",
    "    except KiprisServiceKeyError as e:\n",
    "        print(\"❌ KIPRIS 서비스키/기간/권한 오류:\", str(e))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f88bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 순차 수집 시작: (거절-only 기본) 타겟 → 선행기술(특허/비특허) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae31e678ece4bf1b9517419a9e05bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sequential collecting (targets→GT):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch KR101612025 B1: HTTP error / not found\n",
      "Failed to fetch JP04190910 A: HTTP error / not found\n",
      "Failed to fetch KR101576106 B1: HTTP error / not found\n",
      "Failed to fetch KR101830056 B1: HTTP error / not found\n",
      "Failed to fetch KR101932494 B1: HTTP error / not found\n",
      "Failed to fetch KR101431429 B1: HTTP error / not found\n",
      "Failed to fetch KR101002358 B1: HTTP error / not found\n",
      "Failed to fetch KR101649173 B1: HTTP error / not found\n",
      "Failed to fetch KR101783449 B1: HTTP error / not found\n",
      "Failed to fetch KR101002358 B1: HTTP error / not found\n",
      "Failed to fetch KR101649173 B1: HTTP error / not found\n",
      "Failed to fetch KR101783449 B1: HTTP error / not found\n",
      "Failed to fetch JP07244787 A: HTTP error / not found\n",
      "Failed to fetch KR101803658 B1: HTTP error / not found\n",
      "Failed to fetch US06594024 B1: HTTP error / not found\n",
      "Failed to fetch KR101686919 B1: HTTP error / not found\n",
      "Failed to fetch KR102032521 B1: HTTP error / not found\n",
      "Failed to fetch KR101857624 B1: HTTP error / not found\n",
      "Failed to fetch KR100708319 B1: HTTP error / not found\n",
      "Failed to fetch KR101163561 B1: HTTP error / not found\n",
      "Failed to fetch KR101762543 B1: HTTP error / not found\n",
      "Failed to fetch KR102040506 B1: HTTP error / not found\n",
      "Failed to fetch KR101863196 B1: HTTP error / not found\n",
      "Failed to fetch KR101972055 B1: HTTP error / not found\n",
      "Failed to fetch EP02594905 A2: HTTP error / not found\n",
      "Failed to fetch KR101935750 B1: HTTP error / not found\n",
      "Failed to fetch KR102008611 B1: HTTP error / not found\n",
      "Failed to fetch KR101977258 B1: HTTP error / not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch KR102080712 B1: HTTP error / not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch KR102050917 B1: HTTP error / not found\n",
      "Failed to fetch KR101948236 B1: HTTP error / not found\n",
      "Failed to fetch KR102050298 B1: HTTP error / not found\n",
      "Failed to fetch KR101937434 B1: HTTP error / not found\n",
      "Failed to fetch KR101527886 B1: HTTP error / not found\n",
      "Failed to fetch KR101738899 B1: HTTP error / not found\n",
      "Failed to fetch KR101387998 B1: HTTP error / not found\n",
      "Failed to fetch KR102149671 B1: HTTP error / not found\n",
      "Failed to fetch KR101995755 B1: HTTP error / not found\n",
      "Failed to fetch KR102086659 B1: HTTP error / not found\n",
      "Failed to fetch KR101995755 B1: HTTP error / not found\n",
      "Failed to fetch JP60056853 A: HTTP error / not found\n",
      "Failed to fetch KR101965605 B1: HTTP error / not found\n",
      "Failed to fetch KR101508641 B1: HTTP error / not found\n",
      "Failed to fetch KR102241357 B1: HTTP error / not found\n",
      "Failed to fetch KR101989982 B1: HTTP error / not found\n",
      "Failed to fetch KR102211049 B1: HTTP error / not found\n",
      "Failed to fetch KR101734212 B1: HTTP error / not found\n",
      "Failed to fetch KR101762534 B1: HTTP error / not found\n",
      "Failed to fetch KR100729239 B1: HTTP error / not found\n",
      "Failed to fetch KR101874286 B1: HTTP error / not found\n",
      "Failed to fetch KR102280440 B1: HTTP error / not found\n",
      "Failed to fetch KR102219659 B1: HTTP error / not found\n",
      "Failed to fetch KR102288870 B1: HTTP error / not found\n",
      "Failed to fetch KR102144830 B1: HTTP error / not found\n",
      "Failed to fetch KR102300799 B1: HTTP error / not found\n",
      "Failed to fetch KR102335156 B1: HTTP error / not found\n",
      "\n",
      "[순차 수집 집계]\n",
      "- 타겟: 100/100 (cache 0, api 100)\n",
      "- 선행기술(특허): 192 (cache 0, kipris 115, google 77)\n",
      "- 선행기술(NPL): 19 (cache 0, web 0, citation_only 19)\n",
      "- 선행기술(특허) 실패(처음 10개): ['KR101612025 B1', 'JP04190910 A', 'CN108445868 A', 'KR101576106 B1', 'KR101830056 B1', 'KR101932494 B1', 'KR101431429 B1', 'KR101002358 B1', 'KR101649173 B1', 'KR101783449 B1']\n"
     ]
    }
   ],
   "source": [
    "# 순차 수집: 타겟 특허 원문 → 해당 타겟의 GT 선행기술(특허/비특허) 원문(또는 citation/web 텍스트)\n",
    "# [중요] 수집 즉시 파일로 저장하여 중단 시에도 데이터를 보존합니다.\n",
    "import hashlib\n",
    "\n",
    "print(\"\\n=== 순차 수집 시작: (거절-only 기본) 타겟 → 선행기술(특허/비특허) ===\")\n",
    "\n",
    "# 1. 저장 경로 및 함수 정의 (수집 루프에서 즉시 사용)\n",
    "target_dir = OUT_DIR / \"targets\"\n",
    "prior_art_dir = OUT_DIR / \"prior_arts\"\n",
    "npl_dir = OUT_DIR / \"npl\"\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "prior_art_dir.mkdir(parents=True, exist_ok=True)\n",
    "npl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_stem(value: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", str(value))\n",
    "\n",
    "def save_patent_text(data: Dict[str, str], output_path: Path) -> None:\n",
    "    \"\"\"특허 데이터를 텍스트 파일로 저장 (UTF-8)\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        doc_id = data.get(\"application_number\") or data.get(\"doc_number\", \"\")\n",
    "        f.write(f\"Document Number: {doc_id}\\n\")\n",
    "        f.write(f\"Source: {data.get('source', 'unknown')}\\n\")\n",
    "        if data.get(\"kipris_operation\"):\n",
    "            f.write(f\"KIPRIS Operation: {data.get('kipris_operation')}\\n\")\n",
    "        f.write(f\"Title: {data.get('title', '')}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        if data.get(\"abstract\"):\n",
    "            f.write(\"## ABSTRACT\\n\\n\")\n",
    "            f.write(data[\"abstract\"] + \"\\n\\n\")\n",
    "\n",
    "        if data.get(\"description\"):\n",
    "            f.write(\"## DESCRIPTION\\n\\n\")\n",
    "            f.write(data[\"description\"] + \"\\n\\n\")\n",
    "\n",
    "        if data.get(\"claims\"):\n",
    "            f.write(\"## CLAIMS\\n\\n\")\n",
    "            f.write(data[\"claims\"] + \"\\n\\n\")\n",
    "\n",
    "        if data.get(\"full_text\") and len(data.get(\"full_text\", \"\")) > 1000:\n",
    "            f.write(\"## FULL TEXT (FROM PDF)\\n\\n\")\n",
    "            f.write(data[\"full_text\"] + \"\\n\\n\")\n",
    "\n",
    "def save_npl_text(rec: Dict[str, str], output_path: Path) -> None:\n",
    "    \"\"\"비특허(NPL) 데이터를 텍스트로 저장\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(f\"Source: {rec.get('source', 'citation_only')}\\n\")\n",
    "        if rec.get(\"url\"):\n",
    "            f.write(f\"URL: {rec.get('url')}\\n\")\n",
    "        if rec.get(\"citation\"):\n",
    "            f.write(f\"Citation: {rec.get('citation')}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(rec.get(\"text\") or rec.get(\"citation\") or \"\")\n",
    "\n",
    "# 2. 수집 상태 변수\n",
    "target_fulltext: Dict[str, Dict[str, str]] = {}\n",
    "prior_art_fulltext: Dict[str, Dict[str, str]] = {}\n",
    "npl_fulltext: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "failed_targets: List[str] = []\n",
    "failed_target_details: Dict[str, Dict[str, Any]] = {}\n",
    "failed_prior_arts: List[str] = []\n",
    "failed_npl: List[str] = []\n",
    "\n",
    "cache_hit_target = 0\n",
    "api_hit_target = 0\n",
    "cache_hit_prior = 0\n",
    "api_hit_kipris = 0\n",
    "api_hit_google = 0\n",
    "npl_cache_hit = 0\n",
    "npl_web_hit = 0\n",
    "npl_citation_only = 0\n",
    "\n",
    "def _npl_key(npl: str) -> str:\n",
    "    return hashlib.sha1(str(npl).encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def _extract_first_url(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r\"https?://\\S+\", text)\n",
    "    if not m:\n",
    "        return None\n",
    "    url = m.group(0).strip()\n",
    "    url = url.rstrip(\")].,;\\\"'”)」\")\n",
    "    return url if url.startswith(\"http\") else None\n",
    "\n",
    "def fetch_npl_text(npl: str) -> Dict[str, str]:\n",
    "    raw = str(npl).strip()\n",
    "    url = _extract_first_url(raw)\n",
    "    if not url:\n",
    "        return {\"source\": \"citation_only\", \"citation\": raw, \"text\": raw}\n",
    "\n",
    "    try:\n",
    "        _throttle_google()\n",
    "        resp = requests.get(url, timeout=30, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        extracted = soup.get_text(\"\\n\")\n",
    "        extracted = re.sub(r\"\\n{3,}\", \"\\n\\n\", extracted).strip()\n",
    "        if len(extracted) > 300_000:\n",
    "            extracted = extracted[:300_000]\n",
    "        return {\"source\": \"web\", \"url\": url, \"citation\": raw, \"text\": extracted or raw}\n",
    "    except Exception as e:\n",
    "        return {\"source\": \"citation_only\", \"url\": url, \"citation\": raw, \"text\": raw, \"error\": str(e)}\n",
    "\n",
    "def _load_cached_npl(npl: str) -> Optional[Dict[str, str]]:\n",
    "    k = _npl_key(npl)\n",
    "    p = npl_dir / f\"{k}.txt\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    d = load_saved_npl_txt(p)\n",
    "    if not d:\n",
    "        return None\n",
    "    return {\"source\": \"cached_txt\", \"key\": k, \"raw\": d.get(\"raw\", \"\")}\n",
    "\n",
    "# 3. 메인 루프 (타겟 1건씩 처리 후 즉시 저장)\n",
    "for entry in tqdm(sample, desc=\"Sequential collecting (targets→GT)\"):\n",
    "    tp = entry.get(\"target_patent\", {}) or {}\n",
    "    app_num = str(tp.get(\"application_number\", \"\")).strip()\n",
    "    if not app_num:\n",
    "        continue\n",
    "\n",
    "    # --- 1) 타겟 수집 & 저장 ---\n",
    "    target_save_path = target_dir / f\"{_safe_stem(app_num)}.txt\"\n",
    "    \n",
    "    if app_num not in target_fulltext:\n",
    "        cached = (globals().get(\"cached_target_text_by_app\") or {}).get(app_num)\n",
    "        if cached:\n",
    "            target_fulltext[app_num] = cached\n",
    "            cache_hit_target += 1\n",
    "            # 캐시된 내용이라도 파일이 없으면 저장 (복구)\n",
    "            if not target_save_path.exists():\n",
    "                save_patent_text(cached, target_save_path)\n",
    "        else:\n",
    "            # 파일이 이미 존재하면 로드해서 사용 (재개)\n",
    "            if target_save_path.exists():\n",
    "                loaded = load_saved_patent_text_txt(target_save_path)\n",
    "                if loaded:\n",
    "                    target_fulltext[app_num] = loaded\n",
    "                    cache_hit_target += 1\n",
    "                    # (이미 파일 있으므로 저장 스킵)\n",
    "                else:\n",
    "                    # 파일 깨짐 등 -> 다시 수집\n",
    "                    pass\n",
    "            \n",
    "            if app_num not in target_fulltext:\n",
    "                reg_num = (target_reg_num_by_app or {}).get(str(app_num))\n",
    "                try:\n",
    "                    result, info = get_kipris_fulltext_with_reason(app_num, registration_number=reg_num)\n",
    "                except KiprisServiceKeyError as e:\n",
    "                    raise RuntimeError(f\"KIPRIS 서비스키/권한 문제로 중단: {e}\")\n",
    "                api_hit_target += 1\n",
    "                if result:\n",
    "                    target_fulltext[app_num] = result\n",
    "                    cached_target_text_by_app[app_num] = result\n",
    "                    # [즉시 저장]\n",
    "                    save_patent_text(result, target_save_path)\n",
    "                else:\n",
    "                    failed_targets.append(app_num)\n",
    "                    failed_target_details[app_num] = info\n",
    "\n",
    "    # --- 2) GT 선행기술 수집 & 저장 ---\n",
    "    gt_list = [str(x) for x in (entry.get(\"ground_truth_prior_arts\") or [])]\n",
    "    for pa in gt_list:\n",
    "        pa = str(pa).strip()\n",
    "        if not pa:\n",
    "            continue\n",
    "\n",
    "        # 2-A) 특허 문서번호\n",
    "        if looks_like_patent_doc_number(pa):\n",
    "            doc_num = pa\n",
    "            prior_save_path = prior_art_dir / f\"{_safe_stem(doc_num)}.txt\"\n",
    "\n",
    "            if doc_num in prior_art_fulltext:\n",
    "                continue\n",
    "            \n",
    "            cached = (globals().get(\"cached_prior_art_text_by_doc\") or {}).get(doc_num)\n",
    "            if cached:\n",
    "                prior_art_fulltext[doc_num] = cached\n",
    "                cache_hit_prior += 1\n",
    "                if not prior_save_path.exists():\n",
    "                    save_patent_text(cached, prior_save_path)\n",
    "                continue\n",
    "            \n",
    "            if prior_save_path.exists():\n",
    "                loaded = load_saved_patent_text_txt(prior_save_path)\n",
    "                if loaded:\n",
    "                    prior_art_fulltext[doc_num] = loaded\n",
    "                    cache_hit_prior += 1\n",
    "                    continue\n",
    "\n",
    "            # 수집 실행\n",
    "            res = None\n",
    "            if is_korean_patent(doc_num):\n",
    "                res = get_kipris_fulltext(doc_num)\n",
    "                if res:\n",
    "                    api_hit_kipris += 1\n",
    "                else:\n",
    "                    res = get_google_patents_fulltext(doc_num)\n",
    "                    if res: api_hit_google += 1\n",
    "            else:\n",
    "                res = get_google_patents_fulltext(doc_num)\n",
    "                if res: api_hit_google += 1\n",
    "            \n",
    "            if res:\n",
    "                prior_art_fulltext[doc_num] = res\n",
    "                cached_prior_art_text_by_doc[doc_num] = res\n",
    "                # [즉시 저장]\n",
    "                save_patent_text(res, prior_save_path)\n",
    "            else:\n",
    "                failed_prior_arts.append(doc_num)\n",
    "            continue\n",
    "\n",
    "        # 2-B) 비특허(NPL)\n",
    "        npl = pa\n",
    "        k = _npl_key(npl)\n",
    "        npl_save_path = npl_dir / f\"{k}.txt\"\n",
    "\n",
    "        if k in npl_fulltext:\n",
    "            continue\n",
    "        \n",
    "        cached_n = _load_cached_npl(npl)\n",
    "        if cached_n:\n",
    "            npl_fulltext[k] = cached_n\n",
    "            npl_cache_hit += 1\n",
    "            continue\n",
    "        \n",
    "        if npl_save_path.exists():\n",
    "            # 파일 있으면 로드 (단, NPL은 단순 텍스트라 다시 읽기 쉬움)\n",
    "            d = load_saved_npl_txt(npl_save_path)\n",
    "            if d:\n",
    "                npl_fulltext[k] = {\"key\": k, \"source\": \"cached_file\", **d}\n",
    "                npl_cache_hit += 1\n",
    "                continue\n",
    "\n",
    "        rec = fetch_npl_text(npl)\n",
    "        npl_fulltext[k] = {\"key\": k, **rec}\n",
    "        if rec.get(\"source\") == \"web\":\n",
    "            npl_web_hit += 1\n",
    "        else:\n",
    "            npl_citation_only += 1\n",
    "        \n",
    "        # [즉시 저장]\n",
    "        save_npl_text(rec, npl_save_path)\n",
    "\n",
    "        if rec.get(\"error\"):\n",
    "            failed_npl.append(npl)\n",
    "\n",
    "print(\"\\n[순차 수집 집계]\")\n",
    "print(f\"- 타겟: {len(target_fulltext)}/{len(target_app_nums)} (cache {cache_hit_target}, api {api_hit_target})\")\n",
    "print(f\"- 선행기술(특허): {len(prior_art_fulltext)} (cache {cache_hit_prior}, kipris {api_hit_kipris}, google {api_hit_google})\")\n",
    "print(f\"- 선행기술(NPL): {len(npl_fulltext)} (cache {npl_cache_hit}, web {npl_web_hit}, citation_only {npl_citation_only})\")\n",
    "if failed_targets:\n",
    "    print(f\"- 타겟 실패(처음 5개): {failed_targets[:5]}\")\n",
    "if failed_prior_arts:\n",
    "    print(f\"- 선행기술(특허) 실패(처음 10개): {failed_prior_arts[:10]}\")\n",
    "if failed_npl:\n",
    "    print(f\"- NPL 웹 추출 실패(처음 3개, citation-only로 저장됨): {failed_npl[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a1ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 선행기술 수집은 이전 셀에서 entry 단위로 순차 수행되었습니다.\n",
      "- 선행기술(특허) 수집량: 192\n",
      "- 선행기술(NPL) 수집량: 19\n"
     ]
    }
   ],
   "source": [
    "# (호환) 선행기술 수집 셀\n",
    "# - 현재 노트북은 '순차 수집(타겟→GT)'을 기본으로 하므로, 이 셀에서 추가 수집을 수행하지 않습니다.\n",
    "# - prior_art_fulltext / failed_prior_arts / npl_fulltext 등은 이전 셀에서 채워집니다.\n",
    "\n",
    "print(\"\\n[INFO] 선행기술 수집은 이전 셀에서 entry 단위로 순차 수행되었습니다.\")\n",
    "print(f\"- 선행기술(특허) 수집량: {len(globals().get('prior_art_fulltext', {}) or {})}\")\n",
    "print(f\"- 선행기술(NPL) 수집량: {len(globals().get('npl_fulltext', {}) or {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64982f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 메타데이터 및 데이터셋 생성 (파일 저장은 수집 루프에서 완료됨) ===\n",
      "✓ KIPRIS 타겟 실패 사유 저장: /home/arkwith/Dev/paper_data/data/processed/fulltext/kipris_target_failures.json\n",
      "✓ 메타데이터 저장 (UTF-8): /home/arkwith/Dev/paper_data/data/processed/fulltext/fulltext_metadata.json\n",
      "✓ 평가용 JSONL 저장: /home/arkwith/Dev/paper_data/data/processed/fulltext/paper_eval_sample.jsonl (targets=100)\n",
      "- Claim1 누락: 100/100 (100.0%)\n",
      "✓ 원문 확보 데이터셋(JSONL) 저장: /home/arkwith/Dev/paper_data/data/processed/fulltext/paper_fulltext_dataset_sample100.jsonl (records=100)\n",
      "- 타겟 txt 누락: 0\n",
      "- 선행기술 txt 누락(특허): 66\n",
      "- 선행기술 txt 누락(NPL): 0\n"
     ]
    }
   ],
   "source": [
    "# 메타데이터 생성 및 JSONL Export (파일 저장은 이미 완료됨)\n",
    "\n",
    "print(\"\\n=== 메타데이터 및 데이터셋 생성 (파일 저장은 수집 루프에서 완료됨) ===\")\n",
    "\n",
    "# 실패 사유 저장 (타겟)\n",
    "fail_detail_path = OUT_DIR / \"kipris_target_failures.json\"\n",
    "try:\n",
    "    with open(fail_detail_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"failed_targets\": failed_targets,\n",
    "            \"failed_target_details\": failed_target_details,\n",
    "            \"failed_target_reason_summary\": globals().get(\"failed_target_reason_summary\", {}),\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ KIPRIS 타겟 실패 사유 저장: {fail_detail_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 실패 사유 저장 실패: {e}\")\n",
    "\n",
    "# 메타데이터 저장 (UTF-8)\n",
    "metadata = {\n",
    "    \"dataset_path\": str(DATASET_PATH),\n",
    "    \"cohort\": str(globals().get(\"COHORT\", \"unknown\")),\n",
    "    \"sampling_method\": str(globals().get(\"SAMPLING_METHOD\", \"unknown\")),\n",
    "    \"sample_size\": len(sample),\n",
    "    \"target_patents\": {\n",
    "        \"total\": len(target_app_nums),\n",
    "        \"collected\": len(target_fulltext),\n",
    "        \"failed\": len(failed_targets),\n",
    "        \"failed_reason_summary\": globals().get(\"failed_target_reason_summary\", {}),\n",
    "    },\n",
    "    \"prior_arts\": {\n",
    "        \"total_original_unique\": len(set(prior_art_values)),\n",
    "        \"total_patent_like\": len(prior_art_patent_doc_nums),\n",
    "        \"total_npl_like\": len(prior_art_npl_values),\n",
    "        \"collected_patent\": len(prior_art_fulltext),\n",
    "        \"failed_patent\": len(failed_prior_arts),\n",
    "        \"collected_npl\": len(npl_fulltext or {}),\n",
    "        \"failed_npl_web_extract\": len(globals().get(\"failed_npl\", []) or []),\n",
    "        \"korean\": len(korean_prior_arts),\n",
    "        \"foreign\": len(foreign_prior_arts),\n",
    "    },\n",
    "    \"target_to_prior_arts\": {\n",
    "        entry[\"target_patent\"][\"application_number\"]: (entry.get(\"ground_truth_prior_arts\") or [])\n",
    "        for entry in sample\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = OUT_DIR / \"fulltext_metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ 메타데이터 저장 (UTF-8): {metadata_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 논문 평가용 JSONL export\n",
    "# -------------------------\n",
    "def extract_claim1(claims_text: str, *, max_chars: int = 1500) -> str:\n",
    "    \"\"\"Claim 1을 보수적으로 추출 (실패 시 claims 앞부분 fallback).\"\"\"\n",
    "    if not claims_text:\n",
    "        return \"\"\n",
    "    t = str(claims_text).replace(\"\\r\", \"\\n\")\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    patterns = [\n",
    "        r\"\\[\\s*청구항\\s*1\\s*\\]\\s*(.+?)(?=\\[\\s*청구항\\s*2\\s*\\]|\\Z)\",\n",
    "        r\"청구항\\s*1\\s*[:\\.-]?\\s*(.+?)(?=청구항\\s*2|\\Z)\",\n",
    "        r\"(?:^|\\n)\\s*1\\s*[\\.|\\)]\\s*(.+?)(?=(?:\\n\\s*2\\s*[\\.|\\)]|\\Z))\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, t, flags=re.DOTALL)\n",
    "        if m:\n",
    "            c1 = re.sub(r\"\\s+\", \" \", m.group(1).strip()).strip()\n",
    "            return c1[:max_chars]\n",
    "    fallback = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return fallback[:max_chars]\n",
    "\n",
    "def build_query_text(title: str, abstract: str, claim1: str) -> str:\n",
    "    parts: List[str] = []\n",
    "    if title:\n",
    "        parts.append(f\"Title: {title.strip()}\")\n",
    "    if abstract:\n",
    "        parts.append(f\"Abstract: {abstract.strip()}\")\n",
    "    if claim1:\n",
    "        parts.append(f\"Claim 1: {claim1.strip()}\")\n",
    "    return \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "def split_patent_vs_npl(values: List[str]) -> tuple[list[str], list[str]]:\n",
    "    pats: list[str] = []\n",
    "    npls: list[str] = []\n",
    "    for v in values:\n",
    "        if looks_like_patent_doc_number(v):\n",
    "            pats.append(v)\n",
    "        else:\n",
    "            npls.append(v)\n",
    "    return pats, npls\n",
    "\n",
    "eval_records: list[dict] = []\n",
    "missing_claim1 = 0\n",
    "total_targets = 0\n",
    "\n",
    "for entry in sample:\n",
    "    tp = entry.get(\"target_patent\", {}) or {}\n",
    "    app_num = str(tp.get(\"application_number\", \"\")).strip()\n",
    "    if not app_num:\n",
    "        continue\n",
    "    total_targets += 1\n",
    "\n",
    "    title = str(tp.get(\"title\") or \"\").strip()\n",
    "    abstract = str(tp.get(\"abstract\") or \"\").strip()\n",
    "    gt_all = [str(x) for x in (entry.get(\"ground_truth_prior_arts\") or [])]\n",
    "    gt_pat, gt_npl = split_patent_vs_npl(gt_all)\n",
    "\n",
    "    claims_text = (target_fulltext.get(app_num) or {}).get(\"claims\", \"\")\n",
    "    claim1 = extract_claim1(claims_text)\n",
    "    if not claim1:\n",
    "        missing_claim1 += 1\n",
    "\n",
    "    query_text = build_query_text(title, abstract, claim1)\n",
    "\n",
    "    eval_records.append({\n",
    "        \"target_application_number\": app_num,\n",
    "        \"query\": {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"claim1\": claim1,\n",
    "            \"query_text\": query_text,\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"prior_arts_all\": gt_all,\n",
    "            \"prior_arts_patent\": gt_pat,\n",
    "            \"prior_arts_npl\": gt_npl,\n",
    "        },\n",
    "        \"corpus\": {\n",
    "            \"targets_dir\": str(target_dir),\n",
    "            \"prior_arts_dir\": str(prior_art_dir),\n",
    "            \"npl_dir\": str(npl_dir),\n",
    "        },\n",
    "    })\n",
    "\n",
    "eval_path = OUT_DIR / \"paper_eval_sample.jsonl\"\n",
    "with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in eval_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✓ 평가용 JSONL 저장: {eval_path} (targets={len(eval_records)})\")\n",
    "if total_targets:\n",
    "    print(f\"- Claim1 누락: {missing_claim1}/{total_targets} ({missing_claim1/total_targets*100:.1f}%)\")\n",
    "\n",
    "# -------------------------\n",
    "# (추가) 원문 확보 실험용 데이터셋(JSONL) 생성: 샘플 100건 + 파일 경로 + 가용성\n",
    "# -------------------------\n",
    "fulltext_dataset_records: list[dict] = []\n",
    "missing_target_txt = 0\n",
    "missing_prior_txt = 0\n",
    "missing_npl_txt = 0\n",
    "\n",
    "def _npl_key(npl: str) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha1(str(npl).encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "for entry in sample:\n",
    "    tp = entry.get(\"target_patent\", {}) or {}\n",
    "    app_num = str(tp.get(\"application_number\", \"\")).strip()\n",
    "    if not app_num:\n",
    "        continue\n",
    "    target_txt = target_dir / f\"{re.sub(r'[^a-zA-Z0-9_-]', '_', app_num)}.txt\"\n",
    "    if not target_txt.exists():\n",
    "        missing_target_txt += 1\n",
    "\n",
    "    gt_all = [str(x) for x in (entry.get(\"ground_truth_prior_arts\") or [])]\n",
    "    gt_pat, gt_npl = split_patent_vs_npl(gt_all)\n",
    "\n",
    "    prior_txt_paths: list[str] = []\n",
    "    for doc in gt_pat:\n",
    "        p = prior_art_dir / f\"{re.sub(r'[^a-zA-Z0-9_-]', '_', str(doc))}.txt\"\n",
    "        if p.exists():\n",
    "            prior_txt_paths.append(str(p))\n",
    "        else:\n",
    "            missing_prior_txt += 1\n",
    "\n",
    "    npl_txt_paths: list[str] = []\n",
    "    for npl in gt_npl:\n",
    "        p = npl_dir / f\"{_npl_key(npl)}.txt\"\n",
    "        if p.exists():\n",
    "            npl_txt_paths.append(str(p))\n",
    "        else:\n",
    "            missing_npl_txt += 1\n",
    "\n",
    "    fulltext_dataset_records.append({\n",
    "        \"target\": {\n",
    "            \"application_number\": app_num,\n",
    "            \"title\": str(tp.get(\"title\") or \"\").strip(),\n",
    "            \"abstract\": str(tp.get(\"abstract\") or \"\").strip(),\n",
    "            \"registration_number\": (target_reg_num_by_app or {}).get(app_num, \"\"),\n",
    "            \"text_path\": str(target_txt),\n",
    "            \"text_exists\": target_txt.exists(),\n",
    "            \"kipris_operation\": (target_fulltext.get(app_num) or {}).get(\"kipris_operation\", \"\"),\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"prior_arts_all\": gt_all,\n",
    "            \"prior_arts_patent\": gt_pat,\n",
    "            \"prior_arts_npl\": gt_npl,\n",
    "            \"prior_art_text_paths\": prior_txt_paths,\n",
    "            \"npl_text_paths\": npl_txt_paths,\n",
    "        },\n",
    "        \"dataset_source_path\": str(DATASET_PATH),\n",
    "    })\n",
    "\n",
    "fulltext_dataset_path = OUT_DIR / f\"paper_fulltext_dataset_sample{len(sample)}.jsonl\"\n",
    "with open(fulltext_dataset_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in fulltext_dataset_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✓ 원문 확보 데이터셋(JSONL) 저장: {fulltext_dataset_path} (records={len(fulltext_dataset_records)})\")\n",
    "print(f\"- 타겟 txt 누락: {missing_target_txt}\")\n",
    "print(f\"- 선행기술 txt 누락(특허): {missing_prior_txt}\")\n",
    "print(f\"- 선행기술 txt 누락(NPL): {missing_npl_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ab3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ 특허 원문 수집 완료\n",
      "============================================================\n",
      "\n",
      "📊 타겟 특허 (한국)\n",
      "  - 대상: 100건\n",
      "  - 성공: 100건 (100.0%)\n",
      "  - 실패: 0건\n",
      "  - 소스: KIPRIS PDF API\n",
      "\n",
      "📊 선행기술\n",
      "  - 원본(중복 제거): 273건\n",
      "  - 특허로 판별됨: 254건\n",
      "  - NPL로 판별됨(스킵): 19건\n",
      "  - 수집 성공(특허만): 192건 (75.6%)\n",
      "  - 실패(특허만): 66건\n",
      "\n",
      "📈 선행기술 소스 분포:\n",
      "  - 한국(KIPRIS PDF): 115건\n",
      "  - Google Patents: 77건\n",
      "\n",
      "💾 저장 위치\n",
      "  - 타겟 텍스트: /home/arkwith/Dev/paper_data/data/processed/fulltext/targets\n",
      "  - 선행기술 텍스트: /home/arkwith/Dev/paper_data/data/processed/fulltext/prior_arts\n",
      "  - PDF 캐시: /home/arkwith/Dev/paper_data/data/processed/fulltext_pdfs\n",
      "  - 메타데이터: /home/arkwith/Dev/paper_data/data/processed/fulltext/fulltext_metadata.json\n",
      "\n",
      "✅ 다음 단계: RAG 벡터스토어 구축\n",
      "  → 텍스트 청킹 및 임베딩 생성\n",
      "  → DB 저장 및 키워드 색인\n",
      "\n",
      "🔍 UTF-8 인코딩 검증 (샘플 파일):\n",
      "  파일: KR1020190097397_A.txt\n",
      "  내용 미리보기: Document Number: KR1020190097397 A\n",
      "Source: kipris_pdf\n",
      "KIPRIS Operation: getPubFullTextInfoSearch\n",
      "Title: \n",
      "===============...\n",
      "  ✓ 다국어 문자 정상 감지됨 (한국어: True, 중국어: False, 일본어: False)\n"
     ]
    }
   ],
   "source": [
    "# 수집 결과 요약\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ 특허 원문 수집 완료\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 타겟 특허 (한국)\")\n",
    "print(f\"  - 대상: {len(target_app_nums)}건\")\n",
    "print(f\"  - 성공: {len(target_fulltext)}건 ({len(target_fulltext)/len(target_app_nums)*100:.1f}%)\")\n",
    "print(f\"  - 실패: {len(failed_targets)}건\")\n",
    "print(f\"  - 소스: KIPRIS PDF API\")\n",
    "\n",
    "print(f\"\\n📊 선행기술\")\n",
    "print(f\"  - 원본(중복 제거): {len(set(prior_art_values))}건\")\n",
    "print(f\"  - 특허로 판별됨: {len(prior_art_patent_doc_nums)}건\")\n",
    "print(f\"  - NPL로 판별됨(스킵): {len(prior_art_npl_values)}건\")\n",
    "\n",
    "if len(prior_art_patent_doc_nums) > 0:\n",
    "    print(f\"  - 수집 성공(특허만): {len(prior_art_fulltext)}건 ({len(prior_art_fulltext)/len(prior_art_patent_doc_nums)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"  - 수집 성공(특허만): {len(prior_art_fulltext)}건\")\n",
    "\n",
    "print(f\"  - 실패(특허만): {len(failed_prior_arts)}건\")\n",
    "\n",
    "# 소스별 분포\n",
    "kipris_count = len([v for v in prior_art_fulltext.values() if v.get('source') == 'kipris_pdf'])\n",
    "google_count = len([v for v in prior_art_fulltext.values() if v.get('source') == 'google_patents'])\n",
    "\n",
    "print(f\"\\n📈 선행기술 소스 분포:\")\n",
    "print(f\"  - 한국(KIPRIS PDF): {kipris_count}건\")\n",
    "print(f\"  - Google Patents: {google_count}건\")\n",
    "\n",
    "print(f\"\\n💾 저장 위치\")\n",
    "print(f\"  - 타겟 텍스트: {target_dir}\")\n",
    "print(f\"  - 선행기술 텍스트: {prior_art_dir}\")\n",
    "print(f\"  - PDF 캐시: {PDF_DIR}\")\n",
    "print(f\"  - 메타데이터: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n✅ 다음 단계: RAG 벡터스토어 구축\")\n",
    "print(f\"  → 텍스트 청킹 및 임베딩 생성\")\n",
    "print(f\"  → DB 저장 및 키워드 색인\")\n",
    "\n",
    "# 인코딩 검증 (샘플 파일)\n",
    "sample_file = list(prior_art_dir.glob(\"*.txt\"))\n",
    "if sample_file:\n",
    "    sample_file = sample_file[0]\n",
    "    with open(sample_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()[:300]\n",
    "\n",
    "    print(f\"\\n🔍 UTF-8 인코딩 검증 (샘플 파일):\")\n",
    "    print(f\"  파일: {sample_file.name}\")\n",
    "    print(f\"  내용 미리보기: {content[:120]}...\")\n",
    "\n",
    "    has_korean = bool(re.search(r\"[가-힣]\", content))\n",
    "    has_chinese = bool(re.search(r\"[\\u4e00-\\u9fff]\", content))\n",
    "    has_japanese = bool(re.search(r\"[\\u3040-\\u309f\\u30a0-\\u30ff]\", content))\n",
    "\n",
    "    if has_korean or has_chinese or has_japanese:\n",
    "        print(f\"  ✓ 다국어 문자 정상 감지됨 (한국어: {has_korean}, 중국어: {has_chinese}, 일본어: {has_japanese})\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 샘플에서 CJK 문자가 감지되지 않았습니다. (문서가 영어일 수 있음)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
